// Repository: tigergraph-vector-embeddings
// ├── README.md
// ├── .env.example
// ├── docker-compose.yml
// ├── Makefile
// ├── licenses/
// │   └── enterprise-license.txt   # put your EE license here (not committed)
// ├── gsql/
// │   ├── schema.gsql
// │   ├── loading_jobs.gsql
// │   └── queries.gsql
// ├── scripts/
// │   ├── init_tigergraph.sh
// │   └── wait_for_tg.sh
// ├── worker/
// │   ├── Dockerfile
// │   ├── requirements.txt
// │   ├── ingest_docs.py
// │   ├── .env.example
// │   └── sample_docs/
// │       ├── example1.txt
// │       └── example2.md


// ===============================
// File: README.md
// ===============================
/*
# TigerGraph + Vector Embeddings (Containerized Example)

Production-minded, repeatable Docker setup for **TigerGraph Enterprise** running locally, with a Python worker that embeds documents (Sentence-Transformers) and loads them into TigerGraph. Includes GSQL schema, loaders, and a cosine-similarity query for ANN-style retrieval.

> ⚠️ You need a valid **TigerGraph Enterprise** license to run EE locally. Place it at `licenses/enterprise-license.txt`.

## What you get
- **Docker Compose** for TigerGraph EE + a lightweight **embeddings worker**.
- **GSQL schema**: `Document` vertex holding text + `embedding` (LIST<FLOAT> of size 384 by default), relationships optional.
- **Loading jobs**: load from CSV via RESTPP/REST loader.
- **Similarity query**: top-K cosine similarity over stored embeddings (brute-force demo that you can later optimize).
- **Scripts**: automated init, license apply, user/password, token, graph creation, query install.

## Quick Start

```bash
# 0) Clone and enter
git clone https://example.com/tigergraph-vector-embeddings.git
cd tigergraph-vector-embeddings

# 1) Copy examples
cp .env.example .env
cp worker/.env.example worker/.env

# 2) Put your EE license
#    Place the license file at: licenses/enterprise-license.txt

# 3) Launch containers
docker compose up -d --build

# 4) Initialize TigerGraph (license, password, graph, schema, queries)
make init

# 5) Ingest sample docs (creates embeddings locally and pushes to TG)
make ingest

# 6) Run a similarity query (find docs similar to a given text)
make query Q="quick brown fox"
```

## Ports & Credentials
- TigerGraph RESTPP: `localhost:9000`
- TigerGraph GSQL/Studio: `localhost:14240`
- Default credentials (dev only): `tigergraph / tigergraph`

## Structure
See repository tree at the top of this file.

## Operational Notes
- Embeddings model: `sentence-transformers/all-MiniLM-L6-v2` (384 dims).
- You can change model/dim via env: `EMBED_MODEL`, `EMBED_DIM`.
- For larger graphs, consider:
  - Precomputing IVF/flat indexes outside TG and storing references.
  - Or using TigerGraph vector indexing/UDFs if available in your version.
  - Sharding/splitting embeddings across vertices.

## Common Make Targets
```bash
make up         # compose up -d
make down       # compose down -v
make logs       # tail TG logs
make init       # run init_tigergraph.sh inside container
make ingest     # run worker to embed + load sample docs
make query Q="your text here" # cosine topK
```

## Cleanup
```bash
make down
rm -rf data/   # if you mapped persistent volumes
```

*/

// ===============================
// File: .env.example
// ===============================
/*
# --- TigerGraph ---
TG_CONTAINER_NAME=tigergraph
TG_USER=tigergraph
TG_PASSWORD=tigergraph
TG_GRAPH=DocGraph
TG_TOKEN_TTL=2592000
TG_REST_PORT=9000
TG_GSQL_PORT=14240
TG_LICENSE_FILE=./licenses/enterprise-license.txt

# --- Worker / Embeddings ---
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBED_DIM=384
BATCH_SIZE=16
MAX_TOKENS=2048

# --- Networking ---
NETWORK_NAME=tgnet
*/

// ===============================
// File: docker-compose.yml
// ===============================
/*
services:
  tigergraph:
    image: tigergraph/tigergraph:latest
    container_name: ${TG_CONTAINER_NAME}
    environment:
      - USER=${TG_USER}
      - PASSWORD=${TG_PASSWORD}
      - TG_PASSPHRASE=${TG_PASSWORD}
    ports:
      - "${TG_REST_PORT}:9000"     # RESTPP
      - "${TG_GSQL_PORT}:14240"    # GSQL/Studio
      - "14022:22"                 # SSH (optional)
    volumes:
      - tgdata:/home/tigergraph/data
      - ./gsql:/opt/gsql
      - ./scripts:/opt/scripts
      - ${TG_LICENSE_FILE}:/home/tigergraph/enterprise-license.txt:ro
    healthcheck:
      test: ["CMD", "/opt/scripts/wait_for_tg.sh"]
      interval: 15s
      timeout: 10s
      retries: 20
    networks:
      - tgnet

  worker:
    build:
      context: ./worker
      dockerfile: Dockerfile
    environment:
      - TG_HOST=tigergraph
      - TG_REST_PORT=9000
      - TG_GSQL_PORT=14240
      - TG_USER=${TG_USER}
      - TG_PASSWORD=${TG_PASSWORD}
      - TG_GRAPH=${TG_GRAPH}
      - EMBED_MODEL=${EMBED_MODEL}
      - EMBED_DIM=${EMBED_DIM}
      - BATCH_SIZE=${BATCH_SIZE}
      - MAX_TOKENS=${MAX_TOKENS}
    volumes:
      - ./worker/sample_docs:/app/sample_docs:ro
    depends_on:
      tigergraph:
        condition: service_healthy
    networks:
      - tgnet

volumes:
  tgdata:

networks:
  tgnet:
    name: ${NETWORK_NAME}
*/

// ===============================
// File: Makefile
// ===============================
/*
SHELL := /bin/bash
include .env

.PHONY: up down logs init ingest query

up:
	docker compose up -d --build

e2e: up init ingest

logs:
	docker logs -f $${TG_CONTAINER_NAME}

down:
	docker compose down -v

init:
	@echo "[init] Applying license, creating graph, schema, queries"
	docker exec -it $${TG_CONTAINER_NAME} bash -lc "/opt/scripts/init_tigergraph.sh"

ingest:
	@echo "[ingest] Embedding and loading sample docs"
	docker compose run --rm worker python /app/ingest_docs.py --dir /app/sample_docs --upsert

query:
	@[ -z "$(Q)" ] && echo "Usage: make query Q='text to search'" && exit 1 || true
	docker compose run --rm worker python /app/ingest_docs.py --query "$(Q)" --topk 5
*/

// ===============================
// File: scripts/wait_for_tg.sh
// ===============================
/*
#!/usr/bin/env bash
set -euo pipefail

# Return 0 (healthy) when REST++ responds
if curl -fsS http://localhost:9000/echo >/dev/null 2>&1; then
  exit 0
else
  exit 1
fi
*/

// ===============================
// File: scripts/init_tigergraph.sh
// ===============================
/*
#!/usr/bin/env bash
set -euo pipefail

USER=${USER:-tigergraph}
PASS=${PASSWORD:-tigergraph}
GRAPH=${TG_GRAPH:-DocGraph}
TOKEN_TTL=${TG_TOKEN_TTL:-2592000}
LICENSE_PATH=/home/tigergraph/enterprise-license.txt

GREEN='\033[0;32m'; NC='\033[0m'

say(){ echo -e "${GREEN}[init]${NC} $*"; }

say "Start TigerGraph services"
su - tigergraph -c "gadmin start all" || true
sleep 5

say "Wait for REST++"
/opt/scripts/wait_for_tg.sh || true

if [[ -f "$LICENSE_PATH" ]]; then
  say "Apply Enterprise license"
  su - tigergraph -c "gadmin license set -f $LICENSE_PATH || true"
  su - tigergraph -c "gadmin restart all || true"
  sleep 5
fi

say "Set password for user ${USER}"
su - tigergraph -c "echo -e '${PASS}\n${PASS}' | passwd tigergraph || true"

say "Create graph ${GRAPH} (idempotent)"
su - tigergraph -c "gsql -g $GRAPH 'ls'" >/dev/null 2>&1 || {
  su - tigergraph -c "gsql 'create graph $GRAPH()'"
}

say "Install schema"
su - tigergraph -c "gsql -g $GRAPH /opt/gsql/schema.gsql"

say "Install loading jobs"
su - tigergraph -c "gsql -g $GRAPH /opt/gsql/loading_jobs.gsql"

say "Install queries"
su - tigergraph -c "gsql -g $GRAPH /opt/gsql/queries.gsql"

say "Create secret and token"
REST_HOST=localhost
REST_PORT=${TG_REST_PORT:-9000}
SECRET=$(curl -s -X POST "http://$REST_HOST:$REST_PORT/requestsecret" | jq -r .secret || echo "")
TOKEN=$(curl -s -X POST "http://$REST_HOST:$REST_PORT/requesttoken?secret=${SECRET}&lifetime=${TOKEN_TTL}" | jq -r .token || echo "")

say "Secret: ${SECRET:0:6}...  Token: ${TOKEN:0:12}..."

say "Done."
*/

// ===============================
// File: gsql/schema.gsql
// ===============================
/*
USE GRAPH DocGraph

# Document vertex with text & vector embedding (384 dims default)
# Adjust size if you change EMBED_DIM

CREATE VERTEX Document (
  PRIMARY_ID id STRING,
  uri STRING,
  title STRING,
  text STRING,
  embedding LIST<FLOAT>    # store vector
) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="TRUE"

# (Optional) Concept vertex for tagging
CREATE VERTEX Concept (
  PRIMARY_ID name STRING
) WITH PRIMARY_ID_AS_ATTRIBUTE="TRUE"

CREATE UNDIRECTED EDGE HasConcept (FROM Document, TO Concept)

# Run only once; subsequent runs should be idempotent via gsql guards in scripts
*/

// ===============================
// File: gsql/loading_jobs.gsql
// ===============================
/*
USE GRAPH DocGraph

# Data source definitions
CREATE DATA SOURCE DocCSVSource FOR LOCAL FILE "docs.csv";

# Loader: upsert documents with embedding list
CREATE LOADING JOB load_docs FOR GRAPH DocGraph {
  DEFINE FIELDS (
    id, uri, title, text, embedding
  );

  LOAD DocCSVSource TO VERTEX Document VALUES (
    id, uri, title, text,
    embedding
  ) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE='"', ESCAPE='\\', MAXLINELENGTH=2000000, ACCUMULATE="true", FLAG="eol"
;
}
*/

// ===============================
// File: gsql/queries.gsql
// ===============================
/*
USE GRAPH DocGraph

# Cosine similarity helper - computes cos between input vector and each doc's embedding
# For demo purposes, this is a brute-force scan. For scale, consider UDFs or vector indexes if available.

CREATE QUERY TopKSimilar( LIST<FLOAT> q, INT topk ) FOR GRAPH DocGraph SYNTAX v2 { 
  SumAccum<FLOAT> @dot;
  SumAccum<FLOAT> @norm;
  SumAccum<FLOAT> @qnorm;
  FLOAT qnorm = 0.0;
  INT n = q.size();
  INT i = 0;

  # Precompute qnorm
  while i < n do
    qnorm = qnorm + q[i]*q[i];
    i = i + 1;
  end;
  qnorm = sqrt(qnorm);

  Start = {Document.*};

  Start = SELECT d FROM Start:d POST-ACCUM
    d.@dot  = 0.0,
    d.@norm = 0.0,
    d.@qnorm = qnorm,
    i = 0,
    WHILE i < n DO
      CASE WHEN i < d.embedding.size() THEN
        d.@dot  += d.embedding[i] * q[i];
        d.@norm += d.embedding[i] * d.embedding[i];
      END,
      i = i + 1
    END
  ;

  Start = SELECT d FROM Start:d
    ACCUM d.@norm = sqrt(d.@norm)
    ORDER BY CASE WHEN (d.@norm * d.@qnorm) == 0 THEN 0 ELSE (d.@dot / (d.@norm * d.@qnorm)) END DESC
    LIMIT topk
  ;

  PRINT Start[Start.id AS id, Start.title AS title, Start.uri AS uri,
              (CASE WHEN (Start.@norm * Start.@qnorm)==0 THEN 0 ELSE Start.@dot/(Start.@norm * Start.@qnorm) END) AS score];
}

INSTALL QUERY TopKSimilar
*/

// ===============================
// File: worker/Dockerfile
// ===============================
/*
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt /app/
RUN pip install --no-cache-dir -r requirements.txt

COPY ingest_docs.py /app/ingest_docs.py
COPY sample_docs /app/sample_docs

ENV PYTHONUNBUFFERED=1
CMD ["python", "/app/ingest_docs.py", "--help"]
*/

// ===============================
// File: worker/requirements.txt
// ===============================
/*
requests
pandas
numpy
sentence-transformers
pyyaml
tqdm
*/

// ===============================
// File: worker/.env.example
// ===============================
/*
TG_HOST=tigergraph
TG_REST_PORT=9000
TG_GSQL_PORT=14240
TG_USER=tigergraph
TG_PASSWORD=tigergraph
TG_GRAPH=DocGraph
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBED_DIM=384
BATCH_SIZE=16
MAX_TOKENS=2048
*/

// ===============================
// File: worker/sample_docs/example1.txt
// ===============================
/*
The quick brown fox jumps over the lazy dog. Graph databases excel at modeling relationships.
*/

// ===============================
// File: worker/sample_docs/example2.md
// ===============================
/*
# Vector Search 101

Embeddings map semantically similar text to nearby points in a vector space. Cosine similarity is a common metric.
*/

// ===============================
// File: worker/ingest_docs.py
// ===============================
/*
#!/usr/bin/env python3
import os, io, json, csv, argparse, logging, math
from pathlib import Path
from typing import List, Tuple

import requests
import numpy as np
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

TG_HOST = os.getenv("TG_HOST", "tigergraph")
TG_REST_PORT = int(os.getenv("TG_REST_PORT", "9000"))
TG_USER = os.getenv("TG_USER", "tigergraph")
TG_PASSWORD = os.getenv("TG_PASSWORD", "tigergraph")
TG_GRAPH = os.getenv("TG_GRAPH", "DocGraph")

EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
EMBED_DIM = int(os.getenv("EMBED_DIM", "384"))
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "16"))

REST_BASE = f"http://{TG_HOST}:{TG_REST_PORT}"


def _get_secret_and_token(ttl: int = 2592000) -> Tuple[str, str]:
    s = requests.post(f"{REST_BASE}/requestsecret", timeout=15)
    s.raise_for_status()
    secret = s.json().get("secret")
    t = requests.post(f"{REST_BASE}/requesttoken", params={"secret": secret, "lifetime": ttl}, timeout=15)
    t.raise_for_status()
    token = t.json().get("token")
    return secret, token


def _embed_texts(texts: List[str]) -> List[List[float]]:
    model = SentenceTransformer(EMBED_MODEL)
    vecs = model.encode(texts, batch_size=BATCH_SIZE, show_progress_bar=True, normalize_embeddings=False)
    out = []
    for v in vecs:
        v = v.tolist()
        if len(v) != EMBED_DIM:
            raise ValueError(f"Embedding dim mismatch: got {len(v)} expected {EMBED_DIM}")
        out.append(v)
    return out


def _read_dir(path: Path) -> List[Tuple[str, str, str]]:
    files = []
    for p in sorted(path.rglob("*")):
        if p.is_file() and p.suffix.lower() in {".txt", ".md"}:
            text = p.read_text(encoding="utf-8", errors="ignore")
            files.append((p.stem, str(p), text))
    return files


def _write_docs_csv(rows: List[dict], csv_path: Path) -> None:
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    with csv_path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["id","uri","title","text","embedding"])
        w.writeheader()
        for r in rows:
            # embedding as JSON string so loader can parse list
            r2 = dict(r)
            r2["embedding"] = json.dumps(r2["embedding"])  # serialize list
            w.writerow(r2)


def load_via_rest_loader(csv_path: Path, token: str):
    # Local file load using REST loader: we send file content as multipart
    files = {"file": (csv_path.name, csv_path.read_bytes(), "text/csv")}
    params = {"graph": TG_GRAPH, "tag": "docs", "filename": csv_path.name}
    headers = {"Authorization": f"Bearer {token}"}
    url = f"{REST_BASE}/ddl/loader?job=load_docs"
    logging.info("POST %s", url)
    r = requests.post(url, headers=headers, params=params, files=files, timeout=60)
    if r.status_code >= 300:
        logging.error("Loader error: %s", r.text[:500])
        r.raise_for_status()
    logging.info("Loader response: %s", r.text[:200])


def _topk_query(vec: List[float], topk: int, token: str):
    url = f"{REST_BASE}/query/{TG_GRAPH}/TopKSimilar"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload = {"q": vec, "topk": topk}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    r.raise_for_status()
    return r.json()


def main():
    ap = argparse.ArgumentParser(description="Embed docs and load to TigerGraph")
    ap.add_argument("--dir", default="/app/sample_docs", help="Directory with .txt/.md")
    ap.add_argument("--csv-out", default="/app/docs.csv", help="Intermediary CSV path")
    ap.add_argument("--upsert", action="store_true", help="Run loader to upsert")
    ap.add_argument("--query", default=None, help="Text to search (skip ingest)")
    ap.add_argument("--topk", type=int, default=5)
    args = ap.parse_args()

    secret, token = _get_secret_and_token()
    logging.info("Token acquired: %s...", token[:12])

    if args.query:
        logging.info("Embedding ad-hoc query text")
        vec = _embed_texts([args.query])[0]
        res = _topk_query(vec, args.topk, token)
        print(json.dumps(res, indent=2))
        return

    docs = _read_dir(Path(args.dir))
    if not docs:
        logging.warning("No docs found in %s", args.dir)
        return

    ids = [d[0] for d in docs]
    uris = [d[1] for d in docs]
    titles = [Path(u).name for u in uris]
    texts = [d[2] for d in docs]

    logging.info("Embedding %d docs with %s", len(texts), EMBED_MODEL)
    vecs = _embed_texts(texts)

    rows = []
    for i in range(len(docs)):
        rows.append({
            "id": ids[i],
            "uri": uris[i],
            "title": titles[i],
            "text": texts[i],
            "embedding": vecs[i]
        })

    csv_path = Path(args.csv_out)
    _write_docs_csv(rows, csv_path)
    logging.info("Wrote %s", csv_path)

    if args.upsert:
        load_via_rest_loader(csv_path, token)
        logging.info("Upsert complete")

if __name__ == "__main__":
    main()
*/
